{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f125e1ab208>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    X = np.array(x);\n",
    "    X = X /255.0\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    y = np.zeros([len(x), 10])\n",
    "    for i,j in enumerate(x):\n",
    "        y[i, j] = 1\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    "If you're finding it hard to dedicate enough time for this course a week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) to build each layer, except \"Convolutional & Max Pooling\" layer.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    "If you would like to get the most of this course, try to solve all the problems without TF Layers.  Let's begin!\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function    \n",
    "    return  tf.placeholder(tf.float32, shape=[None, image_shape[0],  image_shape[1],  image_shape[2]], name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=[None, n_classes], name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "Note: You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer.  You're free to use any TensorFlow package for all the other layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    strides =[1, conv_strides[0], conv_strides[1], 1];\n",
    "    weights = tf.Variable(tf.truncated_normal([conv_ksize[0],conv_ksize[1], x_tensor.get_shape()[3].value, conv_num_outputs], mean=0, stddev=0.1));\n",
    "    padding = 'SAME';\n",
    "    bias = tf.Variable(tf.random_normal([conv_num_outputs]))\n",
    "    conv  = tf.nn.conv2d(x_tensor, weights, strides, padding)\n",
    "    conv  = tf.nn.bias_add(conv, bias)\n",
    "    conv  = tf.nn.relu(conv)\n",
    "    \n",
    "    conv = tf.nn.max_pool(conv, ksize=[1, pool_ksize[0],pool_ksize[1], 1], strides=[1,pool_strides[0],pool_strides[1],1], padding='SAME')\n",
    "    return conv \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.reshape(x_tensor, [-1, x_tensor.shape[1].value * x_tensor.shape[2].value* x_tensor.shape[3].value])\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    weight= tf.Variable(tf.truncated_normal([x_tensor.shape[1].value, num_outputs], mean=0.0, stddev=0.1))\n",
    "    bias =  tf.Variable(tf.random_normal([num_outputs]))\n",
    "    \n",
    "    return tf.nn.bias_add(tf.matmul(x_tensor, weight), bias)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer.\n",
    "\n",
    "Note: Activation, softmax, or cross entropy shouldn't be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    weight= tf.Variable(tf.truncated_normal([x_tensor.shape[1].value, num_outputs], mean=0.0, stddev=0.1))\n",
    "    bias =  tf.Variable(tf.random_normal([num_outputs]))\n",
    "    \n",
    "    return tf.nn.bias_add(tf.matmul(x_tensor, weight), bias)\n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv1 = conv2d_maxpool(x, 18, [5,5], [1,1], [2,2], [2,2])\n",
    "    conv2 = conv2d_maxpool(conv1, 48, conv_ksize=[5,5], conv_strides=[1,1], pool_ksize=[2,2], pool_strides=[2,2])\n",
    "    conv2 = tf.nn.dropout(conv2, keep_prob+0.2)\n",
    "    \n",
    "    conv_out = conv2\n",
    "    \n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    fl0 = flatten(conv_out)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    fl1 = fully_conn(fl0, 360)\n",
    "    fl1 = tf.nn.relu(fl1)\n",
    "    fl1 = tf.nn.dropout(fl1, keep_prob)\n",
    "    \n",
    "    fl2 = fully_conn(fl1, 252)\n",
    "    fl2= tf.nn.relu(fl2)\n",
    "    fl2 = tf.nn.dropout(fl2, keep_prob)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    out = output(fl2,10)\n",
    "    \n",
    "    \n",
    "    # TODO: return output\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer,feed_dict={x:feature_batch, y:label_batch, keep_prob: keep_probability})\n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={x:feature_batch, y: label_batch, keep_prob:1.0 })\n",
    "    valid_acc =session.run(accuracy, feed_dict={x:feature_batch, y: label_batch, keep_prob:1.0})\n",
    "    print('Loss: {:>10.4f}  Validation Accuracy: {:.6f}'.format(loss, valid_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2226  Validation Accuracy: 0.200000\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.1416  Validation Accuracy: 0.175000\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     2.0894  Validation Accuracy: 0.275000\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     2.1041  Validation Accuracy: 0.300000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.9565  Validation Accuracy: 0.350000\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.9689  Validation Accuracy: 0.375000\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.8298  Validation Accuracy: 0.450000\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.8207  Validation Accuracy: 0.400000\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.7702  Validation Accuracy: 0.425000\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.7065  Validation Accuracy: 0.450000\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.6219  Validation Accuracy: 0.450000\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.5671  Validation Accuracy: 0.400000\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.6460  Validation Accuracy: 0.525000\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.5163  Validation Accuracy: 0.500000\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.5241  Validation Accuracy: 0.500000\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.4024  Validation Accuracy: 0.575000\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.3443  Validation Accuracy: 0.525000\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.2920  Validation Accuracy: 0.575000\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.2887  Validation Accuracy: 0.500000\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.2315  Validation Accuracy: 0.600000\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.1856  Validation Accuracy: 0.625000\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.1243  Validation Accuracy: 0.650000\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     0.9696  Validation Accuracy: 0.750000\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     1.0055  Validation Accuracy: 0.700000\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     0.9376  Validation Accuracy: 0.725000\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     0.8088  Validation Accuracy: 0.725000\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     0.7533  Validation Accuracy: 0.775000\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     0.7201  Validation Accuracy: 0.850000\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     0.7504  Validation Accuracy: 0.775000\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     0.6975  Validation Accuracy: 0.775000\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     0.6476  Validation Accuracy: 0.775000\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     0.5670  Validation Accuracy: 0.800000\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     0.5453  Validation Accuracy: 0.800000\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     0.4746  Validation Accuracy: 0.850000\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     0.4840  Validation Accuracy: 0.875000\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     0.4217  Validation Accuracy: 0.950000\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     0.4727  Validation Accuracy: 0.900000\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     0.4112  Validation Accuracy: 0.925000\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     0.4269  Validation Accuracy: 0.900000\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     0.2998  Validation Accuracy: 0.975000\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     0.2864  Validation Accuracy: 0.950000\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     0.3167  Validation Accuracy: 0.925000\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     0.2925  Validation Accuracy: 0.925000\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     0.2781  Validation Accuracy: 0.950000\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     0.2187  Validation Accuracy: 0.925000\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     0.1779  Validation Accuracy: 0.950000\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     0.1607  Validation Accuracy: 0.975000\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     0.2318  Validation Accuracy: 0.900000\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     0.1619  Validation Accuracy: 0.975000\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     0.1934  Validation Accuracy: 0.975000\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     0.1212  Validation Accuracy: 0.975000\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     0.1673  Validation Accuracy: 0.950000\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     0.1365  Validation Accuracy: 0.975000\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     0.1253  Validation Accuracy: 0.975000\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     0.0779  Validation Accuracy: 1.000000\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     0.0893  Validation Accuracy: 1.000000\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.0745  Validation Accuracy: 1.000000\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.1062  Validation Accuracy: 0.975000\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.0868  Validation Accuracy: 1.000000\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.1165  Validation Accuracy: 1.000000\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.0580  Validation Accuracy: 1.000000\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.0486  Validation Accuracy: 1.000000\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.1215  Validation Accuracy: 1.000000\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.0474  Validation Accuracy: 1.000000\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.0459  Validation Accuracy: 1.000000\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.0325  Validation Accuracy: 1.000000\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.0705  Validation Accuracy: 0.975000\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.0412  Validation Accuracy: 1.000000\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.0285  Validation Accuracy: 1.000000\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.0348  Validation Accuracy: 1.000000\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.0272  Validation Accuracy: 1.000000\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.0302  Validation Accuracy: 1.000000\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.0271  Validation Accuracy: 1.000000\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.0213  Validation Accuracy: 1.000000\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.0184  Validation Accuracy: 1.000000\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.0268  Validation Accuracy: 1.000000\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.0104  Validation Accuracy: 1.000000\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.0194  Validation Accuracy: 1.000000\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.0131  Validation Accuracy: 1.000000\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.0074  Validation Accuracy: 1.000000\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.0167  Validation Accuracy: 1.000000\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.0072  Validation Accuracy: 1.000000\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.0060  Validation Accuracy: 1.000000\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.0095  Validation Accuracy: 1.000000\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.0099  Validation Accuracy: 1.000000\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.0088  Validation Accuracy: 1.000000\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.0083  Validation Accuracy: 1.000000\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.0176  Validation Accuracy: 1.000000\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.0076  Validation Accuracy: 1.000000\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.0070  Validation Accuracy: 1.000000\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.0053  Validation Accuracy: 1.000000\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.0060  Validation Accuracy: 1.000000\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.0046  Validation Accuracy: 1.000000\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.0048  Validation Accuracy: 1.000000\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.0098  Validation Accuracy: 1.000000\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.0071  Validation Accuracy: 1.000000\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.0065  Validation Accuracy: 1.000000\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.0042  Validation Accuracy: 1.000000\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.0050  Validation Accuracy: 1.000000\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.0017  Validation Accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2359  Validation Accuracy: 0.250000\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.2070  Validation Accuracy: 0.175000\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     1.8952  Validation Accuracy: 0.225000\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     1.8688  Validation Accuracy: 0.300000\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     1.8853  Validation Accuracy: 0.325000\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     1.9832  Validation Accuracy: 0.375000\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     1.7932  Validation Accuracy: 0.350000\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     1.5040  Validation Accuracy: 0.500000\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     1.5672  Validation Accuracy: 0.450000\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     1.6817  Validation Accuracy: 0.475000\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.7705  Validation Accuracy: 0.350000\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     1.6391  Validation Accuracy: 0.400000\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     1.3821  Validation Accuracy: 0.400000\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     1.4738  Validation Accuracy: 0.450000\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     1.5629  Validation Accuracy: 0.425000\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.5834  Validation Accuracy: 0.450000\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     1.3421  Validation Accuracy: 0.550000\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     1.2716  Validation Accuracy: 0.450000\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     1.3363  Validation Accuracy: 0.425000\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     1.4657  Validation Accuracy: 0.500000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.4927  Validation Accuracy: 0.425000\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     1.2612  Validation Accuracy: 0.500000\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     1.1798  Validation Accuracy: 0.650000\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     1.3147  Validation Accuracy: 0.575000\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     1.3884  Validation Accuracy: 0.500000\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.3959  Validation Accuracy: 0.450000\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     1.1275  Validation Accuracy: 0.575000\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     1.1154  Validation Accuracy: 0.525000\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     1.1949  Validation Accuracy: 0.575000\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     1.3417  Validation Accuracy: 0.525000\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.2919  Validation Accuracy: 0.475000\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     1.0701  Validation Accuracy: 0.625000\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     1.0577  Validation Accuracy: 0.625000\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     1.1457  Validation Accuracy: 0.600000\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     1.2440  Validation Accuracy: 0.525000\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.3716  Validation Accuracy: 0.500000\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     1.0663  Validation Accuracy: 0.525000\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     1.0143  Validation Accuracy: 0.575000\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     1.1152  Validation Accuracy: 0.625000\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     1.1805  Validation Accuracy: 0.600000\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.2402  Validation Accuracy: 0.500000\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     1.0368  Validation Accuracy: 0.600000\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     0.9437  Validation Accuracy: 0.650000\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     1.0577  Validation Accuracy: 0.600000\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     1.1058  Validation Accuracy: 0.650000\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.1875  Validation Accuracy: 0.500000\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     1.0124  Validation Accuracy: 0.675000\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     0.9330  Validation Accuracy: 0.725000\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     0.9808  Validation Accuracy: 0.625000\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     1.0952  Validation Accuracy: 0.650000\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.2407  Validation Accuracy: 0.550000\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     1.0436  Validation Accuracy: 0.575000\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     0.9138  Validation Accuracy: 0.600000\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     0.8446  Validation Accuracy: 0.750000\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     1.0469  Validation Accuracy: 0.725000\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.0471  Validation Accuracy: 0.650000\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     0.9067  Validation Accuracy: 0.650000\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     0.9029  Validation Accuracy: 0.675000\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     0.8571  Validation Accuracy: 0.700000\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     1.0305  Validation Accuracy: 0.675000\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.0486  Validation Accuracy: 0.550000\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     0.8292  Validation Accuracy: 0.725000\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     0.8855  Validation Accuracy: 0.750000\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     0.7354  Validation Accuracy: 0.800000\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     1.0151  Validation Accuracy: 0.675000\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.1178  Validation Accuracy: 0.600000\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     0.8583  Validation Accuracy: 0.725000\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     0.8558  Validation Accuracy: 0.725000\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     0.6647  Validation Accuracy: 0.775000\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     0.9055  Validation Accuracy: 0.800000\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     0.9626  Validation Accuracy: 0.675000\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     0.8042  Validation Accuracy: 0.675000\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     0.7475  Validation Accuracy: 0.725000\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     0.6684  Validation Accuracy: 0.750000\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     0.9476  Validation Accuracy: 0.750000\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.0031  Validation Accuracy: 0.675000\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     0.7415  Validation Accuracy: 0.725000\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     0.7546  Validation Accuracy: 0.825000\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     0.6445  Validation Accuracy: 0.775000\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     0.8389  Validation Accuracy: 0.775000\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     0.8965  Validation Accuracy: 0.700000\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     0.7008  Validation Accuracy: 0.825000\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     0.7496  Validation Accuracy: 0.775000\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     0.5980  Validation Accuracy: 0.800000\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     0.7355  Validation Accuracy: 0.800000\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     0.8411  Validation Accuracy: 0.700000\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     0.6664  Validation Accuracy: 0.725000\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     0.6389  Validation Accuracy: 0.825000\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     0.5902  Validation Accuracy: 0.800000\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     0.8549  Validation Accuracy: 0.750000\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     0.8033  Validation Accuracy: 0.775000\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     0.6462  Validation Accuracy: 0.775000\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     0.5633  Validation Accuracy: 0.850000\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     0.6453  Validation Accuracy: 0.750000\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     0.7528  Validation Accuracy: 0.775000\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     0.8339  Validation Accuracy: 0.750000\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     0.5999  Validation Accuracy: 0.750000\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     0.5949  Validation Accuracy: 0.825000\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     0.5092  Validation Accuracy: 0.850000\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     0.7650  Validation Accuracy: 0.825000\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     0.7392  Validation Accuracy: 0.775000\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:     0.5496  Validation Accuracy: 0.800000\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:     0.5103  Validation Accuracy: 0.875000\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:     0.5038  Validation Accuracy: 0.750000\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:     0.6474  Validation Accuracy: 0.800000\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     0.7246  Validation Accuracy: 0.700000\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:     0.6215  Validation Accuracy: 0.825000\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:     0.5213  Validation Accuracy: 0.850000\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:     0.4649  Validation Accuracy: 0.850000\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:     0.6560  Validation Accuracy: 0.725000\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     0.7813  Validation Accuracy: 0.750000\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:     0.5291  Validation Accuracy: 0.900000\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:     0.3954  Validation Accuracy: 0.900000\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:     0.4429  Validation Accuracy: 0.800000\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:     0.5798  Validation Accuracy: 0.900000\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     0.7202  Validation Accuracy: 0.750000\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:     0.5578  Validation Accuracy: 0.850000\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:     0.4097  Validation Accuracy: 0.875000\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:     0.4007  Validation Accuracy: 0.925000\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:     0.6042  Validation Accuracy: 0.850000\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     0.7636  Validation Accuracy: 0.700000\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:     0.5300  Validation Accuracy: 0.825000\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:     0.4123  Validation Accuracy: 0.875000\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:     0.3997  Validation Accuracy: 0.900000\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:     0.5467  Validation Accuracy: 0.950000\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     0.6463  Validation Accuracy: 0.750000\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:     0.4833  Validation Accuracy: 0.850000\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:     0.3142  Validation Accuracy: 0.950000\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:     0.3815  Validation Accuracy: 0.900000\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:     0.4994  Validation Accuracy: 0.825000\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     0.6849  Validation Accuracy: 0.825000\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:     0.5324  Validation Accuracy: 0.800000\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:     0.3461  Validation Accuracy: 0.925000\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:     0.3424  Validation Accuracy: 0.900000\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:     0.4731  Validation Accuracy: 0.850000\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     0.5401  Validation Accuracy: 0.825000\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:     0.4769  Validation Accuracy: 0.900000\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:     0.3311  Validation Accuracy: 0.925000\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:     0.3108  Validation Accuracy: 1.000000\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:     0.4011  Validation Accuracy: 0.925000\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     0.6133  Validation Accuracy: 0.875000\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:     0.4460  Validation Accuracy: 0.875000\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:     0.3132  Validation Accuracy: 0.950000\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:     0.4008  Validation Accuracy: 0.875000\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:     0.4635  Validation Accuracy: 0.975000\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     0.5996  Validation Accuracy: 0.850000\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:     0.3886  Validation Accuracy: 0.900000\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:     0.2777  Validation Accuracy: 0.925000\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:     0.2838  Validation Accuracy: 0.925000\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:     0.4770  Validation Accuracy: 0.875000\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     0.4281  Validation Accuracy: 0.875000\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:     0.3970  Validation Accuracy: 0.875000\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:     0.2836  Validation Accuracy: 0.950000\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:     0.2955  Validation Accuracy: 0.950000\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:     0.3965  Validation Accuracy: 0.950000\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     0.4566  Validation Accuracy: 0.900000\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:     0.3545  Validation Accuracy: 0.875000\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:     0.2189  Validation Accuracy: 0.950000\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:     0.2756  Validation Accuracy: 0.975000\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:     0.3887  Validation Accuracy: 0.925000\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     0.3845  Validation Accuracy: 0.850000\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:     0.3325  Validation Accuracy: 0.950000\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:     0.2576  Validation Accuracy: 0.975000\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:     0.3121  Validation Accuracy: 0.900000\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:     0.3394  Validation Accuracy: 0.925000\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     0.3248  Validation Accuracy: 0.900000\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:     0.2665  Validation Accuracy: 0.950000\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:     0.2206  Validation Accuracy: 0.950000\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:     0.3345  Validation Accuracy: 0.950000\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:     0.3154  Validation Accuracy: 0.950000\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     0.3493  Validation Accuracy: 0.925000\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:     0.2634  Validation Accuracy: 0.925000\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:     0.2005  Validation Accuracy: 0.975000\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:     0.2690  Validation Accuracy: 0.925000\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:     0.2801  Validation Accuracy: 0.950000\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     0.4091  Validation Accuracy: 0.850000\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:     0.2788  Validation Accuracy: 0.975000\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:     0.2266  Validation Accuracy: 0.950000\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:     0.2599  Validation Accuracy: 0.975000\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:     0.2760  Validation Accuracy: 0.950000\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     0.3457  Validation Accuracy: 0.875000\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:     0.2804  Validation Accuracy: 0.900000\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:     0.2101  Validation Accuracy: 0.950000\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:     0.2244  Validation Accuracy: 0.950000\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:     0.2971  Validation Accuracy: 0.975000\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     0.2852  Validation Accuracy: 0.900000\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:     0.2443  Validation Accuracy: 0.925000\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:     0.1937  Validation Accuracy: 0.975000\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:     0.2512  Validation Accuracy: 0.975000\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:     0.2724  Validation Accuracy: 1.000000\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     0.3125  Validation Accuracy: 0.850000\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:     0.2818  Validation Accuracy: 0.975000\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:     0.1954  Validation Accuracy: 0.950000\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:     0.2259  Validation Accuracy: 0.925000\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:     0.2792  Validation Accuracy: 0.975000\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     0.2645  Validation Accuracy: 0.950000\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:     0.2933  Validation Accuracy: 0.900000\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:     0.1729  Validation Accuracy: 0.975000\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:     0.2265  Validation Accuracy: 0.975000\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:     0.2587  Validation Accuracy: 0.950000\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     0.3703  Validation Accuracy: 0.900000\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:     0.2458  Validation Accuracy: 0.975000\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:     0.1550  Validation Accuracy: 1.000000\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:     0.2034  Validation Accuracy: 1.000000\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:     0.2160  Validation Accuracy: 0.950000\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     0.2752  Validation Accuracy: 0.925000\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:     0.1994  Validation Accuracy: 0.950000\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:     0.1554  Validation Accuracy: 0.975000\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:     0.1580  Validation Accuracy: 1.000000\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:     0.2425  Validation Accuracy: 1.000000\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     0.3751  Validation Accuracy: 0.900000\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:     0.1896  Validation Accuracy: 0.975000\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:     0.1527  Validation Accuracy: 0.975000\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:     0.1616  Validation Accuracy: 1.000000\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:     0.2113  Validation Accuracy: 0.900000\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     0.2194  Validation Accuracy: 0.925000\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss:     0.2036  Validation Accuracy: 0.950000\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:     0.1527  Validation Accuracy: 0.975000\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:     0.1698  Validation Accuracy: 0.975000\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:     0.2517  Validation Accuracy: 0.975000\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     0.2410  Validation Accuracy: 0.925000\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:     0.1562  Validation Accuracy: 0.975000\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:     0.1570  Validation Accuracy: 1.000000\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:     0.2000  Validation Accuracy: 1.000000\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:     0.1938  Validation Accuracy: 1.000000\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     0.2104  Validation Accuracy: 1.000000\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:     0.1626  Validation Accuracy: 1.000000\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:     0.1103  Validation Accuracy: 0.950000\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:     0.1553  Validation Accuracy: 1.000000\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:     0.1897  Validation Accuracy: 1.000000\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     0.2483  Validation Accuracy: 0.900000\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:     0.1708  Validation Accuracy: 0.975000\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:     0.1166  Validation Accuracy: 1.000000\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:     0.1820  Validation Accuracy: 1.000000\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:     0.1840  Validation Accuracy: 0.950000\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     0.1763  Validation Accuracy: 0.925000\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:     0.1121  Validation Accuracy: 1.000000\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:     0.1337  Validation Accuracy: 0.975000\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:     0.1973  Validation Accuracy: 0.950000\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:     0.1795  Validation Accuracy: 0.975000\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     0.2140  Validation Accuracy: 0.925000\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:     0.1313  Validation Accuracy: 1.000000\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:     0.1014  Validation Accuracy: 1.000000\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:     0.2035  Validation Accuracy: 0.975000\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:     0.2034  Validation Accuracy: 0.975000\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     0.2033  Validation Accuracy: 0.950000\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:     0.1433  Validation Accuracy: 0.975000\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:     0.1477  Validation Accuracy: 0.975000\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:     0.1521  Validation Accuracy: 1.000000\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:     0.1887  Validation Accuracy: 0.975000\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     0.1932  Validation Accuracy: 1.000000\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:     0.1164  Validation Accuracy: 1.000000\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:     0.0892  Validation Accuracy: 1.000000\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss:     0.1337  Validation Accuracy: 1.000000\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:     0.2109  Validation Accuracy: 1.000000\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     0.1536  Validation Accuracy: 0.975000\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:     0.1365  Validation Accuracy: 1.000000\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:     0.1188  Validation Accuracy: 0.975000\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:     0.1350  Validation Accuracy: 0.975000\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:     0.1687  Validation Accuracy: 1.000000\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     0.1634  Validation Accuracy: 1.000000\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss:     0.1539  Validation Accuracy: 0.975000\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:     0.1016  Validation Accuracy: 1.000000\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:     0.1500  Validation Accuracy: 1.000000\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:     0.1398  Validation Accuracy: 1.000000\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     0.1736  Validation Accuracy: 0.975000\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss:     0.1208  Validation Accuracy: 1.000000\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:     0.1156  Validation Accuracy: 1.000000\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:     0.1019  Validation Accuracy: 1.000000\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:     0.1444  Validation Accuracy: 0.975000\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     0.1596  Validation Accuracy: 0.950000\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:     0.0909  Validation Accuracy: 1.000000\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:     0.0839  Validation Accuracy: 1.000000\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:     0.1363  Validation Accuracy: 1.000000\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:     0.1550  Validation Accuracy: 0.975000\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     0.2247  Validation Accuracy: 0.950000\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:     0.0731  Validation Accuracy: 1.000000\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:     0.0919  Validation Accuracy: 0.975000\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:     0.1170  Validation Accuracy: 1.000000\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:     0.1598  Validation Accuracy: 1.000000\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.1851  Validation Accuracy: 0.950000\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:     0.0820  Validation Accuracy: 1.000000\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:     0.0967  Validation Accuracy: 1.000000\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:     0.1137  Validation Accuracy: 1.000000\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:     0.1490  Validation Accuracy: 1.000000\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.1624  Validation Accuracy: 0.975000\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:     0.0971  Validation Accuracy: 1.000000\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:     0.1136  Validation Accuracy: 0.975000\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:     0.1108  Validation Accuracy: 0.975000\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:     0.1231  Validation Accuracy: 1.000000\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.1250  Validation Accuracy: 1.000000\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:     0.0875  Validation Accuracy: 1.000000\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:     0.0876  Validation Accuracy: 1.000000\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:     0.1073  Validation Accuracy: 0.975000\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:     0.0989  Validation Accuracy: 1.000000\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.1398  Validation Accuracy: 0.975000\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:     0.1022  Validation Accuracy: 0.975000\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:     0.0654  Validation Accuracy: 1.000000\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:     0.0888  Validation Accuracy: 1.000000\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:     0.1094  Validation Accuracy: 0.975000\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.1341  Validation Accuracy: 1.000000\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:     0.0834  Validation Accuracy: 1.000000\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:     0.0880  Validation Accuracy: 1.000000\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:     0.1339  Validation Accuracy: 0.950000\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:     0.1039  Validation Accuracy: 1.000000\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.1419  Validation Accuracy: 1.000000\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:     0.1299  Validation Accuracy: 1.000000\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:     0.0575  Validation Accuracy: 1.000000\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:     0.1448  Validation Accuracy: 1.000000\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:     0.0851  Validation Accuracy: 1.000000\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.1383  Validation Accuracy: 1.000000\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:     0.1259  Validation Accuracy: 0.975000\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:     0.0915  Validation Accuracy: 1.000000\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:     0.0926  Validation Accuracy: 1.000000\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:     0.0919  Validation Accuracy: 0.975000\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.1561  Validation Accuracy: 1.000000\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:     0.0944  Validation Accuracy: 1.000000\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:     0.0619  Validation Accuracy: 1.000000\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:     0.1224  Validation Accuracy: 0.975000\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:     0.1277  Validation Accuracy: 1.000000\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.1161  Validation Accuracy: 1.000000\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss:     0.0651  Validation Accuracy: 1.000000\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:     0.0737  Validation Accuracy: 1.000000\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss:     0.0890  Validation Accuracy: 0.975000\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss:     0.0977  Validation Accuracy: 1.000000\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.1214  Validation Accuracy: 1.000000\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:     0.0882  Validation Accuracy: 1.000000\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss:     0.0782  Validation Accuracy: 1.000000\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:     0.1247  Validation Accuracy: 1.000000\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:     0.1045  Validation Accuracy: 1.000000\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.1164  Validation Accuracy: 1.000000\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:     0.1047  Validation Accuracy: 1.000000\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:     0.0651  Validation Accuracy: 0.975000\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:     0.1065  Validation Accuracy: 1.000000\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:     0.0975  Validation Accuracy: 1.000000\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.0889  Validation Accuracy: 1.000000\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:     0.1266  Validation Accuracy: 1.000000\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:     0.0631  Validation Accuracy: 1.000000\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:     0.0824  Validation Accuracy: 1.000000\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:     0.1102  Validation Accuracy: 1.000000\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.1032  Validation Accuracy: 0.975000\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:     0.0843  Validation Accuracy: 1.000000\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:     0.0806  Validation Accuracy: 0.975000\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:     0.0980  Validation Accuracy: 1.000000\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:     0.0729  Validation Accuracy: 1.000000\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.1093  Validation Accuracy: 1.000000\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:     0.0924  Validation Accuracy: 0.975000\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:     0.0626  Validation Accuracy: 1.000000\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:     0.0931  Validation Accuracy: 1.000000\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:     0.1034  Validation Accuracy: 1.000000\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.0983  Validation Accuracy: 1.000000\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:     0.0562  Validation Accuracy: 1.000000\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:     0.0646  Validation Accuracy: 1.000000\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:     0.0649  Validation Accuracy: 1.000000\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:     0.0606  Validation Accuracy: 1.000000\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.1075  Validation Accuracy: 1.000000\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:     0.1139  Validation Accuracy: 1.000000\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:     0.0565  Validation Accuracy: 1.000000\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:     0.0765  Validation Accuracy: 0.975000\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:     0.0645  Validation Accuracy: 0.975000\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.1224  Validation Accuracy: 1.000000\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:     0.0692  Validation Accuracy: 1.000000\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:     0.0760  Validation Accuracy: 1.000000\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:     0.0727  Validation Accuracy: 1.000000\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:     0.0879  Validation Accuracy: 1.000000\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.1078  Validation Accuracy: 1.000000\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:     0.0862  Validation Accuracy: 1.000000\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:     0.0330  Validation Accuracy: 1.000000\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:     0.0524  Validation Accuracy: 1.000000\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:     0.1238  Validation Accuracy: 0.975000\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.0623  Validation Accuracy: 1.000000\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:     0.0413  Validation Accuracy: 1.000000\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:     0.0682  Validation Accuracy: 1.000000\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:     0.0992  Validation Accuracy: 1.000000\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:     0.1031  Validation Accuracy: 1.000000\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.1347  Validation Accuracy: 1.000000\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:     0.0422  Validation Accuracy: 1.000000\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:     0.0544  Validation Accuracy: 1.000000\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:     0.0801  Validation Accuracy: 1.000000\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:     0.0913  Validation Accuracy: 1.000000\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.0598  Validation Accuracy: 1.000000\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:     0.0462  Validation Accuracy: 1.000000\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:     0.0518  Validation Accuracy: 1.000000\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:     0.0839  Validation Accuracy: 1.000000\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:     0.0996  Validation Accuracy: 1.000000\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.0929  Validation Accuracy: 1.000000\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:     0.0609  Validation Accuracy: 1.000000\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:     0.0646  Validation Accuracy: 1.000000\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:     0.0951  Validation Accuracy: 0.975000\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:     0.0892  Validation Accuracy: 1.000000\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.1101  Validation Accuracy: 0.975000\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:     0.0686  Validation Accuracy: 1.000000\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:     0.0454  Validation Accuracy: 1.000000\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:     0.0848  Validation Accuracy: 1.000000\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:     0.1197  Validation Accuracy: 0.975000\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.0719  Validation Accuracy: 1.000000\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:     0.0639  Validation Accuracy: 1.000000\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss:     0.0425  Validation Accuracy: 1.000000\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:     0.0609  Validation Accuracy: 1.000000\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:     0.0764  Validation Accuracy: 1.000000\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.1143  Validation Accuracy: 1.000000\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss:     0.0594  Validation Accuracy: 1.000000\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss:     0.0545  Validation Accuracy: 1.000000\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss:     0.0732  Validation Accuracy: 1.000000\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss:     0.0851  Validation Accuracy: 0.950000\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.0499  Validation Accuracy: 1.000000\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss:     0.0542  Validation Accuracy: 1.000000\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss:     0.0482  Validation Accuracy: 1.000000\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss:     0.0412  Validation Accuracy: 1.000000\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss:     0.0708  Validation Accuracy: 1.000000\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.0611  Validation Accuracy: 1.000000\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss:     0.0792  Validation Accuracy: 0.975000\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss:     0.0542  Validation Accuracy: 1.000000\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss:     0.0573  Validation Accuracy: 1.000000\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss:     0.0782  Validation Accuracy: 1.000000\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.0548  Validation Accuracy: 1.000000\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss:     0.0331  Validation Accuracy: 1.000000\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss:     0.0401  Validation Accuracy: 1.000000\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss:     0.0720  Validation Accuracy: 1.000000\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss:     0.0799  Validation Accuracy: 1.000000\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.1092  Validation Accuracy: 0.950000\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss:     0.0339  Validation Accuracy: 1.000000\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss:     0.0495  Validation Accuracy: 1.000000\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss:     0.0419  Validation Accuracy: 1.000000\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss:     0.0369  Validation Accuracy: 1.000000\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.0903  Validation Accuracy: 0.975000\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss:     0.0424  Validation Accuracy: 1.000000\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss:     0.0711  Validation Accuracy: 0.975000\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss:     0.0540  Validation Accuracy: 1.000000\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss:     0.0649  Validation Accuracy: 1.000000\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.0852  Validation Accuracy: 0.975000\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss:     0.0352  Validation Accuracy: 1.000000\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss:     0.0388  Validation Accuracy: 1.000000\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss:     0.0479  Validation Accuracy: 1.000000\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss:     0.0634  Validation Accuracy: 1.000000\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.1059  Validation Accuracy: 1.000000\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss:     0.0440  Validation Accuracy: 1.000000\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss:     0.0458  Validation Accuracy: 1.000000\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss:     0.0681  Validation Accuracy: 1.000000\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss:     0.0610  Validation Accuracy: 1.000000\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.0469  Validation Accuracy: 1.000000\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss:     0.0503  Validation Accuracy: 1.000000\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss:     0.0414  Validation Accuracy: 1.000000\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss:     0.0537  Validation Accuracy: 1.000000\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss:     0.0587  Validation Accuracy: 1.000000\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.0601  Validation Accuracy: 1.000000\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss:     0.0450  Validation Accuracy: 1.000000\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss:     0.0326  Validation Accuracy: 1.000000\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss:     0.0359  Validation Accuracy: 1.000000\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss:     0.0542  Validation Accuracy: 1.000000\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.0744  Validation Accuracy: 1.000000\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss:     0.0518  Validation Accuracy: 1.000000\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss:     0.0379  Validation Accuracy: 1.000000\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss:     0.0487  Validation Accuracy: 1.000000\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss:     0.0730  Validation Accuracy: 1.000000\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.0683  Validation Accuracy: 1.000000\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss:     0.0335  Validation Accuracy: 1.000000\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss:     0.0507  Validation Accuracy: 1.000000\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss:     0.0393  Validation Accuracy: 1.000000\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss:     0.0547  Validation Accuracy: 1.000000\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.0680  Validation Accuracy: 1.000000\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss:     0.0829  Validation Accuracy: 1.000000\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss:     0.0287  Validation Accuracy: 1.000000\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss:     0.0411  Validation Accuracy: 1.000000\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss:     0.0830  Validation Accuracy: 0.975000\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.0745  Validation Accuracy: 1.000000\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss:     0.0474  Validation Accuracy: 1.000000\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss:     0.0222  Validation Accuracy: 1.000000\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss:     0.0493  Validation Accuracy: 1.000000\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss:     0.0539  Validation Accuracy: 1.000000\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.0504  Validation Accuracy: 1.000000\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss:     0.0447  Validation Accuracy: 1.000000\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss:     0.0363  Validation Accuracy: 1.000000\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss:     0.0548  Validation Accuracy: 1.000000\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss:     0.0358  Validation Accuracy: 1.000000\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.0691  Validation Accuracy: 1.000000\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss:     0.0345  Validation Accuracy: 1.000000\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss:     0.0377  Validation Accuracy: 1.000000\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss:     0.0382  Validation Accuracy: 1.000000\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss:     0.0411  Validation Accuracy: 1.000000\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.0409  Validation Accuracy: 1.000000\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss:     0.0386  Validation Accuracy: 1.000000\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss:     0.0371  Validation Accuracy: 1.000000\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss:     0.0407  Validation Accuracy: 1.000000\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss:     0.0801  Validation Accuracy: 0.975000\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.0555  Validation Accuracy: 1.000000\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss:     0.0335  Validation Accuracy: 1.000000\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss:     0.0324  Validation Accuracy: 1.000000\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss:     0.0499  Validation Accuracy: 1.000000\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss:     0.0432  Validation Accuracy: 1.000000\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.0445  Validation Accuracy: 1.000000\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss:     0.0335  Validation Accuracy: 1.000000\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss:     0.0538  Validation Accuracy: 1.000000\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss:     0.0223  Validation Accuracy: 1.000000\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss:     0.0436  Validation Accuracy: 1.000000\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.0685  Validation Accuracy: 1.000000\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss:     0.0322  Validation Accuracy: 1.000000\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss:     0.0260  Validation Accuracy: 1.000000\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss:     0.0316  Validation Accuracy: 1.000000\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss:     0.0451  Validation Accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.6793391719745223\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/S/I2slLk/auagAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f12549fc9e8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
