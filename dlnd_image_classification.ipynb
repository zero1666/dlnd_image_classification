{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [00:58, 2.91MB/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3086ba7a90>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    X = np.array(x);\n",
    "    X = X /255.0\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    y = np.zeros([len(x), 10])\n",
    "    for i,j in enumerate(x):\n",
    "        y[i, j] = 1\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    "If you're finding it hard to dedicate enough time for this course a week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) to build each layer, except \"Convolutional & Max Pooling\" layer.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    "If you would like to get the most of this course, try to solve all the problems without TF Layers.  Let's begin!\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function    \n",
    "    return  tf.placeholder(tf.float32, shape=[None, image_shape[0],  image_shape[1],  image_shape[2]], name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=[None, n_classes], name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "Note: You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer.  You're free to use any TensorFlow package for all the other layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    strides =[1, conv_strides[0], conv_strides[1], 1];\n",
    "    weights = tf.Variable(tf.truncated_normal([conv_ksize[0],conv_ksize[1], x_tensor.get_shape()[3].value, conv_num_outputs], mean=0, stddev=0.01));\n",
    "    padding = 'SAME';\n",
    "    #bias = tf.Variable(tf.random_normal([conv_num_outputs]))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    conv  = tf.nn.conv2d(x_tensor, weights, strides, padding)\n",
    "    conv  = tf.nn.bias_add(conv, bias)\n",
    "    conv  = tf.nn.relu(conv)\n",
    "    \n",
    "    conv = tf.nn.max_pool(conv, ksize=[1, pool_ksize[0],pool_ksize[1], 1], strides=[1,pool_strides[0],pool_strides[1],1], padding='SAME')\n",
    "    return conv \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.reshape(x_tensor, [-1, x_tensor.shape[1].value * x_tensor.shape[2].value* x_tensor.shape[3].value])\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    weight= tf.Variable(tf.truncated_normal([x_tensor.shape[1].value, num_outputs], mean=0.0, stddev=0.1))\n",
    "    #bias =  tf.Variable(tf.random_normal([num_outputs]))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    \n",
    "    return tf.nn.bias_add(tf.matmul(x_tensor, weight), bias)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer.\n",
    "\n",
    "Note: Activation, softmax, or cross entropy shouldn't be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    weight= tf.Variable(tf.truncated_normal([x_tensor.shape[1].value, num_outputs], mean=0.0, stddev=0.01))\n",
    "    #bias =  tf.Variable(tf.random_normal([num_outputs]))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    \n",
    "    return tf.nn.bias_add(tf.matmul(x_tensor, weight), bias)\n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv1 = conv2d_maxpool(x, 20, [5,5], [1,1], [2,2], [2,2])\n",
    "    conv2 = conv2d_maxpool(conv1, 50, conv_ksize=[3,3], conv_strides=[1,1], pool_ksize=[2,2], pool_strides=[2,2])\n",
    "    conv2 = tf.nn.dropout(conv2, keep_prob)\n",
    "    \n",
    "    conv_out = conv2\n",
    "    \n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    fl0 = flatten(conv_out)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    fl1 = fully_conn(fl0, 100)\n",
    "    fl1 = tf.nn.relu(fl1)\n",
    "    fl1 = tf.nn.dropout(fl1, keep_prob)\n",
    "    \n",
    "    fl2 = fully_conn(fl1, 50)\n",
    "    fl2= tf.nn.relu(fl2)\n",
    "    fl2 = tf.nn.dropout(fl2, keep_prob+0.2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    out = output(fl2,10)\n",
    "    \n",
    "    \n",
    "    # TODO: return output\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer,feed_dict={x:feature_batch, y:label_batch, keep_prob: keep_probability})\n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={x:feature_batch, y: label_batch, keep_prob:1.0 })\n",
    "    valid_acc =session.run(accuracy, feed_dict={x:feature_batch, y: label_batch, keep_prob:1.0})\n",
    "    print('Loss: {:>10.4f}  Validation Accuracy: {:.6f}'.format(loss, valid_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2494  Validation Accuracy: 0.125000\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.2014  Validation Accuracy: 0.200000\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     2.1908  Validation Accuracy: 0.200000\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     2.1918  Validation Accuracy: 0.200000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     2.1373  Validation Accuracy: 0.200000\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     2.1298  Validation Accuracy: 0.200000\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     2.1110  Validation Accuracy: 0.200000\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     2.0573  Validation Accuracy: 0.175000\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     2.0357  Validation Accuracy: 0.275000\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     2.0443  Validation Accuracy: 0.350000\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     2.0078  Validation Accuracy: 0.325000\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     2.0269  Validation Accuracy: 0.300000\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.9439  Validation Accuracy: 0.400000\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.9218  Validation Accuracy: 0.375000\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.9176  Validation Accuracy: 0.475000\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.9179  Validation Accuracy: 0.375000\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.7966  Validation Accuracy: 0.375000\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.8230  Validation Accuracy: 0.425000\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.7570  Validation Accuracy: 0.425000\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.7285  Validation Accuracy: 0.400000\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.7745  Validation Accuracy: 0.375000\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.6236  Validation Accuracy: 0.500000\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     1.6214  Validation Accuracy: 0.400000\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     1.5925  Validation Accuracy: 0.500000\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     1.6110  Validation Accuracy: 0.475000\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     1.5724  Validation Accuracy: 0.475000\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     1.5918  Validation Accuracy: 0.425000\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     1.5333  Validation Accuracy: 0.475000\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     1.4659  Validation Accuracy: 0.575000\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     1.4791  Validation Accuracy: 0.425000\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     1.3886  Validation Accuracy: 0.550000\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     1.3899  Validation Accuracy: 0.525000\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     1.3732  Validation Accuracy: 0.500000\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     1.3690  Validation Accuracy: 0.575000\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     1.3594  Validation Accuracy: 0.550000\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     1.3181  Validation Accuracy: 0.525000\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     1.2891  Validation Accuracy: 0.575000\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     1.2589  Validation Accuracy: 0.475000\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     1.2575  Validation Accuracy: 0.525000\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     1.2451  Validation Accuracy: 0.575000\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     1.2735  Validation Accuracy: 0.525000\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     1.1830  Validation Accuracy: 0.575000\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     1.1879  Validation Accuracy: 0.575000\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     1.2490  Validation Accuracy: 0.575000\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     1.1710  Validation Accuracy: 0.600000\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     1.2107  Validation Accuracy: 0.575000\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     1.1564  Validation Accuracy: 0.650000\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     1.1588  Validation Accuracy: 0.550000\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     1.1572  Validation Accuracy: 0.625000\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     1.1226  Validation Accuracy: 0.625000\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     1.1020  Validation Accuracy: 0.675000\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     1.0868  Validation Accuracy: 0.650000\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     1.0817  Validation Accuracy: 0.650000\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     1.0821  Validation Accuracy: 0.650000\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     1.0498  Validation Accuracy: 0.650000\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     1.0397  Validation Accuracy: 0.700000\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     1.0094  Validation Accuracy: 0.675000\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.9971  Validation Accuracy: 0.700000\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     1.0158  Validation Accuracy: 0.625000\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.9677  Validation Accuracy: 0.725000\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     1.0121  Validation Accuracy: 0.725000\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.9536  Validation Accuracy: 0.650000\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.9675  Validation Accuracy: 0.725000\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.9553  Validation Accuracy: 0.725000\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.9566  Validation Accuracy: 0.675000\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.9343  Validation Accuracy: 0.725000\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.9290  Validation Accuracy: 0.750000\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.9103  Validation Accuracy: 0.725000\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.9283  Validation Accuracy: 0.650000\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.8968  Validation Accuracy: 0.750000\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.9005  Validation Accuracy: 0.700000\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.9038  Validation Accuracy: 0.700000\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.8306  Validation Accuracy: 0.700000\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.8546  Validation Accuracy: 0.675000\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.8428  Validation Accuracy: 0.725000\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.8435  Validation Accuracy: 0.700000\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.8020  Validation Accuracy: 0.675000\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.7796  Validation Accuracy: 0.750000\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.7774  Validation Accuracy: 0.725000\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.7790  Validation Accuracy: 0.775000\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.7868  Validation Accuracy: 0.725000\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.7776  Validation Accuracy: 0.725000\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.7566  Validation Accuracy: 0.775000\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.7268  Validation Accuracy: 0.825000\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.7714  Validation Accuracy: 0.775000\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.7531  Validation Accuracy: 0.700000\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.7265  Validation Accuracy: 0.750000\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.6972  Validation Accuracy: 0.725000\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.7120  Validation Accuracy: 0.750000\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.7019  Validation Accuracy: 0.700000\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.6980  Validation Accuracy: 0.775000\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.7223  Validation Accuracy: 0.750000\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.6505  Validation Accuracy: 0.825000\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.7033  Validation Accuracy: 0.775000\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.6435  Validation Accuracy: 0.850000\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.6671  Validation Accuracy: 0.825000\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.6370  Validation Accuracy: 0.875000\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.6281  Validation Accuracy: 0.825000\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.5930  Validation Accuracy: 0.850000\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.5952  Validation Accuracy: 0.850000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2372  Validation Accuracy: 0.175000\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.1159  Validation Accuracy: 0.250000\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     1.8265  Validation Accuracy: 0.250000\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     1.9403  Validation Accuracy: 0.200000\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     2.0371  Validation Accuracy: 0.300000\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.0901  Validation Accuracy: 0.225000\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     1.8653  Validation Accuracy: 0.300000\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     1.6902  Validation Accuracy: 0.300000\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     1.6970  Validation Accuracy: 0.350000\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     1.8290  Validation Accuracy: 0.400000\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.9584  Validation Accuracy: 0.400000\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     1.6981  Validation Accuracy: 0.450000\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     1.5174  Validation Accuracy: 0.400000\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     1.6707  Validation Accuracy: 0.400000\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     1.7258  Validation Accuracy: 0.375000\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.8978  Validation Accuracy: 0.375000\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     1.6436  Validation Accuracy: 0.450000\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     1.3347  Validation Accuracy: 0.500000\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     1.5465  Validation Accuracy: 0.475000\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     1.7128  Validation Accuracy: 0.350000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.7834  Validation Accuracy: 0.450000\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     1.5324  Validation Accuracy: 0.475000\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     1.3045  Validation Accuracy: 0.500000\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     1.5257  Validation Accuracy: 0.450000\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     1.6566  Validation Accuracy: 0.350000\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.6781  Validation Accuracy: 0.450000\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     1.4960  Validation Accuracy: 0.450000\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     1.2534  Validation Accuracy: 0.625000\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     1.4849  Validation Accuracy: 0.450000\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     1.5666  Validation Accuracy: 0.425000\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.6567  Validation Accuracy: 0.450000\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     1.4982  Validation Accuracy: 0.450000\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     1.2312  Validation Accuracy: 0.525000\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     1.3948  Validation Accuracy: 0.475000\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     1.4844  Validation Accuracy: 0.525000\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.4992  Validation Accuracy: 0.500000\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     1.4430  Validation Accuracy: 0.450000\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     1.1821  Validation Accuracy: 0.600000\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     1.3232  Validation Accuracy: 0.525000\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     1.5045  Validation Accuracy: 0.450000\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.4528  Validation Accuracy: 0.475000\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     1.3198  Validation Accuracy: 0.500000\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     1.1375  Validation Accuracy: 0.675000\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     1.2914  Validation Accuracy: 0.500000\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     1.3866  Validation Accuracy: 0.575000\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.4518  Validation Accuracy: 0.500000\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     1.2343  Validation Accuracy: 0.450000\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     1.0788  Validation Accuracy: 0.600000\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     1.2644  Validation Accuracy: 0.525000\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     1.3806  Validation Accuracy: 0.475000\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.4010  Validation Accuracy: 0.475000\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     1.1698  Validation Accuracy: 0.525000\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     1.0516  Validation Accuracy: 0.625000\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     1.2707  Validation Accuracy: 0.600000\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     1.3157  Validation Accuracy: 0.525000\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.3599  Validation Accuracy: 0.525000\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     1.2074  Validation Accuracy: 0.500000\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     1.0208  Validation Accuracy: 0.675000\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     1.1853  Validation Accuracy: 0.525000\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     1.4117  Validation Accuracy: 0.500000\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.3349  Validation Accuracy: 0.600000\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     1.1597  Validation Accuracy: 0.550000\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     1.0171  Validation Accuracy: 0.600000\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     1.2106  Validation Accuracy: 0.550000\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     1.2947  Validation Accuracy: 0.575000\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.2076  Validation Accuracy: 0.575000\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     1.1071  Validation Accuracy: 0.500000\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     0.9761  Validation Accuracy: 0.700000\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     1.2030  Validation Accuracy: 0.550000\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     1.1791  Validation Accuracy: 0.625000\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.1880  Validation Accuracy: 0.600000\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     1.1040  Validation Accuracy: 0.625000\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     0.9098  Validation Accuracy: 0.700000\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     1.1678  Validation Accuracy: 0.525000\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     1.1658  Validation Accuracy: 0.625000\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.1826  Validation Accuracy: 0.600000\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     1.0819  Validation Accuracy: 0.575000\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     0.9023  Validation Accuracy: 0.725000\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     1.0694  Validation Accuracy: 0.500000\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     1.1970  Validation Accuracy: 0.600000\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.1248  Validation Accuracy: 0.550000\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     1.0210  Validation Accuracy: 0.550000\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     0.8670  Validation Accuracy: 0.700000\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     1.0768  Validation Accuracy: 0.575000\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     1.1513  Validation Accuracy: 0.625000\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.1189  Validation Accuracy: 0.600000\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     1.0376  Validation Accuracy: 0.575000\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     0.8438  Validation Accuracy: 0.725000\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     1.1000  Validation Accuracy: 0.575000\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     1.1739  Validation Accuracy: 0.625000\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.1321  Validation Accuracy: 0.625000\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     1.0323  Validation Accuracy: 0.550000\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     0.9008  Validation Accuracy: 0.725000\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     1.0743  Validation Accuracy: 0.525000\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     1.0968  Validation Accuracy: 0.700000\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.0750  Validation Accuracy: 0.625000\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     0.9619  Validation Accuracy: 0.575000\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     0.8913  Validation Accuracy: 0.675000\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     1.0874  Validation Accuracy: 0.550000\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     1.0529  Validation Accuracy: 0.725000\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.0681  Validation Accuracy: 0.625000\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:     0.9642  Validation Accuracy: 0.625000\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:     0.8364  Validation Accuracy: 0.725000\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:     1.0148  Validation Accuracy: 0.600000\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:     0.9799  Validation Accuracy: 0.725000\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.0604  Validation Accuracy: 0.600000\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:     0.9296  Validation Accuracy: 0.575000\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:     0.7896  Validation Accuracy: 0.700000\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:     0.9731  Validation Accuracy: 0.625000\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:     1.0022  Validation Accuracy: 0.675000\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     1.0198  Validation Accuracy: 0.625000\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:     0.9040  Validation Accuracy: 0.650000\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:     0.8123  Validation Accuracy: 0.775000\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:     1.0161  Validation Accuracy: 0.650000\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:     0.9761  Validation Accuracy: 0.775000\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     0.9990  Validation Accuracy: 0.625000\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:     0.8987  Validation Accuracy: 0.700000\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:     0.7428  Validation Accuracy: 0.800000\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:     1.0154  Validation Accuracy: 0.600000\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:     0.9433  Validation Accuracy: 0.725000\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     1.0085  Validation Accuracy: 0.650000\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:     0.8659  Validation Accuracy: 0.675000\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:     0.7093  Validation Accuracy: 0.750000\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:     0.9243  Validation Accuracy: 0.675000\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:     0.9473  Validation Accuracy: 0.725000\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     0.9832  Validation Accuracy: 0.650000\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:     0.8886  Validation Accuracy: 0.675000\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:     0.7711  Validation Accuracy: 0.750000\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:     0.9372  Validation Accuracy: 0.700000\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:     0.9010  Validation Accuracy: 0.750000\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     1.0854  Validation Accuracy: 0.600000\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:     0.8675  Validation Accuracy: 0.650000\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:     0.7396  Validation Accuracy: 0.725000\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:     1.0215  Validation Accuracy: 0.625000\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:     0.9058  Validation Accuracy: 0.725000\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     0.9924  Validation Accuracy: 0.650000\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:     0.9031  Validation Accuracy: 0.550000\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:     0.6939  Validation Accuracy: 0.725000\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:     0.9212  Validation Accuracy: 0.700000\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:     0.9071  Validation Accuracy: 0.725000\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     0.9497  Validation Accuracy: 0.625000\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:     0.8592  Validation Accuracy: 0.675000\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:     0.6996  Validation Accuracy: 0.750000\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:     0.9380  Validation Accuracy: 0.675000\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:     0.8502  Validation Accuracy: 0.700000\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     0.8743  Validation Accuracy: 0.675000\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:     0.8550  Validation Accuracy: 0.625000\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:     0.7263  Validation Accuracy: 0.750000\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:     0.8936  Validation Accuracy: 0.700000\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:     0.8132  Validation Accuracy: 0.775000\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     0.9263  Validation Accuracy: 0.650000\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:     0.8591  Validation Accuracy: 0.750000\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:     0.7050  Validation Accuracy: 0.750000\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:     0.9091  Validation Accuracy: 0.650000\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:     0.8144  Validation Accuracy: 0.800000\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     0.8771  Validation Accuracy: 0.675000\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:     0.8745  Validation Accuracy: 0.625000\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:     0.6537  Validation Accuracy: 0.800000\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:     0.8576  Validation Accuracy: 0.675000\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:     0.8889  Validation Accuracy: 0.750000\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     0.8721  Validation Accuracy: 0.725000\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:     0.8246  Validation Accuracy: 0.625000\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:     0.6586  Validation Accuracy: 0.800000\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:     0.8588  Validation Accuracy: 0.725000\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:     0.8173  Validation Accuracy: 0.775000\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     0.8732  Validation Accuracy: 0.700000\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:     0.8139  Validation Accuracy: 0.650000\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:     0.6582  Validation Accuracy: 0.850000\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:     0.8349  Validation Accuracy: 0.725000\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:     0.8087  Validation Accuracy: 0.750000\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     0.9060  Validation Accuracy: 0.600000\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:     0.8454  Validation Accuracy: 0.625000\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:     0.6073  Validation Accuracy: 0.850000\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:     0.8491  Validation Accuracy: 0.650000\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:     0.8406  Validation Accuracy: 0.800000\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     0.8547  Validation Accuracy: 0.700000\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:     0.7701  Validation Accuracy: 0.675000\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:     0.6226  Validation Accuracy: 0.850000\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:     0.8341  Validation Accuracy: 0.675000\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:     0.7707  Validation Accuracy: 0.750000\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     0.8775  Validation Accuracy: 0.700000\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:     0.7966  Validation Accuracy: 0.600000\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:     0.6212  Validation Accuracy: 0.775000\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:     0.7670  Validation Accuracy: 0.725000\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:     0.8222  Validation Accuracy: 0.825000\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     0.8690  Validation Accuracy: 0.625000\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:     0.7881  Validation Accuracy: 0.675000\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:     0.6473  Validation Accuracy: 0.825000\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:     0.7731  Validation Accuracy: 0.775000\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:     0.7262  Validation Accuracy: 0.850000\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     0.8767  Validation Accuracy: 0.700000\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:     0.8282  Validation Accuracy: 0.700000\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:     0.6047  Validation Accuracy: 0.800000\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:     0.8041  Validation Accuracy: 0.775000\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:     0.8059  Validation Accuracy: 0.800000\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     0.8656  Validation Accuracy: 0.725000\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:     0.8171  Validation Accuracy: 0.650000\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:     0.5874  Validation Accuracy: 0.825000\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:     0.8192  Validation Accuracy: 0.700000\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:     0.7627  Validation Accuracy: 0.850000\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     0.8940  Validation Accuracy: 0.700000\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:     0.8058  Validation Accuracy: 0.675000\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:     0.5423  Validation Accuracy: 0.825000\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:     0.8109  Validation Accuracy: 0.725000\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:     0.7344  Validation Accuracy: 0.875000\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     0.8377  Validation Accuracy: 0.750000\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:     0.7345  Validation Accuracy: 0.675000\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:     0.5618  Validation Accuracy: 0.825000\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:     0.7658  Validation Accuracy: 0.750000\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:     0.7355  Validation Accuracy: 0.825000\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     0.8096  Validation Accuracy: 0.750000\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:     0.7760  Validation Accuracy: 0.675000\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:     0.5992  Validation Accuracy: 0.825000\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:     0.7439  Validation Accuracy: 0.775000\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:     0.7543  Validation Accuracy: 0.800000\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     0.8236  Validation Accuracy: 0.750000\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss:     0.6905  Validation Accuracy: 0.675000\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:     0.5713  Validation Accuracy: 0.900000\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:     0.7457  Validation Accuracy: 0.725000\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:     0.7071  Validation Accuracy: 0.825000\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     0.7894  Validation Accuracy: 0.725000\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:     0.7228  Validation Accuracy: 0.650000\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:     0.5593  Validation Accuracy: 0.875000\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:     0.7614  Validation Accuracy: 0.725000\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:     0.7105  Validation Accuracy: 0.825000\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     0.7743  Validation Accuracy: 0.775000\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:     0.7267  Validation Accuracy: 0.775000\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:     0.5896  Validation Accuracy: 0.850000\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:     0.6796  Validation Accuracy: 0.800000\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:     0.6959  Validation Accuracy: 0.875000\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     0.7605  Validation Accuracy: 0.800000\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:     0.7431  Validation Accuracy: 0.700000\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:     0.5244  Validation Accuracy: 0.900000\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:     0.7553  Validation Accuracy: 0.725000\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:     0.6791  Validation Accuracy: 0.825000\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     0.8164  Validation Accuracy: 0.700000\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:     0.7313  Validation Accuracy: 0.750000\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:     0.5728  Validation Accuracy: 0.850000\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:     0.7465  Validation Accuracy: 0.750000\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:     0.6818  Validation Accuracy: 0.825000\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     0.7254  Validation Accuracy: 0.775000\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:     0.7112  Validation Accuracy: 0.725000\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:     0.5617  Validation Accuracy: 0.875000\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:     0.6863  Validation Accuracy: 0.800000\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:     0.7120  Validation Accuracy: 0.825000\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     0.7274  Validation Accuracy: 0.800000\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:     0.6745  Validation Accuracy: 0.775000\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:     0.5928  Validation Accuracy: 0.850000\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:     0.7597  Validation Accuracy: 0.775000\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:     0.6550  Validation Accuracy: 0.825000\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     0.7033  Validation Accuracy: 0.800000\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:     0.7196  Validation Accuracy: 0.725000\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:     0.5420  Validation Accuracy: 0.875000\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss:     0.7322  Validation Accuracy: 0.775000\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:     0.6810  Validation Accuracy: 0.800000\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     0.7233  Validation Accuracy: 0.750000\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:     0.7255  Validation Accuracy: 0.775000\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:     0.5281  Validation Accuracy: 0.875000\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:     0.6371  Validation Accuracy: 0.875000\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:     0.6569  Validation Accuracy: 0.800000\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     0.6720  Validation Accuracy: 0.800000\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss:     0.6852  Validation Accuracy: 0.700000\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:     0.5240  Validation Accuracy: 0.875000\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:     0.6917  Validation Accuracy: 0.850000\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:     0.6300  Validation Accuracy: 0.850000\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     0.6776  Validation Accuracy: 0.725000\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss:     0.6958  Validation Accuracy: 0.775000\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:     0.5261  Validation Accuracy: 0.875000\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:     0.6799  Validation Accuracy: 0.775000\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:     0.6262  Validation Accuracy: 0.775000\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     0.6896  Validation Accuracy: 0.800000\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:     0.6496  Validation Accuracy: 0.750000\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:     0.5412  Validation Accuracy: 0.850000\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:     0.6927  Validation Accuracy: 0.800000\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:     0.6067  Validation Accuracy: 0.925000\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     0.7499  Validation Accuracy: 0.775000\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:     0.6420  Validation Accuracy: 0.825000\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:     0.4968  Validation Accuracy: 0.875000\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:     0.6797  Validation Accuracy: 0.800000\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:     0.6028  Validation Accuracy: 0.900000\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.7002  Validation Accuracy: 0.775000\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:     0.6182  Validation Accuracy: 0.850000\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:     0.5019  Validation Accuracy: 0.875000\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:     0.6716  Validation Accuracy: 0.850000\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:     0.6098  Validation Accuracy: 0.850000\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.6974  Validation Accuracy: 0.775000\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:     0.6255  Validation Accuracy: 0.825000\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:     0.5037  Validation Accuracy: 0.925000\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:     0.6620  Validation Accuracy: 0.775000\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:     0.6143  Validation Accuracy: 0.900000\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.6996  Validation Accuracy: 0.750000\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:     0.6427  Validation Accuracy: 0.775000\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:     0.4803  Validation Accuracy: 0.900000\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:     0.6523  Validation Accuracy: 0.825000\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:     0.5825  Validation Accuracy: 0.875000\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.6737  Validation Accuracy: 0.775000\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:     0.6302  Validation Accuracy: 0.725000\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:     0.5010  Validation Accuracy: 0.900000\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:     0.6833  Validation Accuracy: 0.800000\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:     0.6117  Validation Accuracy: 0.875000\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.7084  Validation Accuracy: 0.775000\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:     0.6052  Validation Accuracy: 0.800000\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:     0.5009  Validation Accuracy: 0.875000\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:     0.6367  Validation Accuracy: 0.825000\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:     0.6178  Validation Accuracy: 0.825000\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.7155  Validation Accuracy: 0.725000\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:     0.6136  Validation Accuracy: 0.825000\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:     0.4763  Validation Accuracy: 0.925000\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:     0.6139  Validation Accuracy: 0.850000\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:     0.6307  Validation Accuracy: 0.825000\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.6913  Validation Accuracy: 0.800000\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:     0.6426  Validation Accuracy: 0.800000\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:     0.5084  Validation Accuracy: 0.850000\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:     0.6583  Validation Accuracy: 0.850000\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:     0.6156  Validation Accuracy: 0.850000\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.6902  Validation Accuracy: 0.725000\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:     0.6062  Validation Accuracy: 0.850000\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:     0.4995  Validation Accuracy: 0.925000\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:     0.6297  Validation Accuracy: 0.775000\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:     0.6266  Validation Accuracy: 0.875000\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.6488  Validation Accuracy: 0.800000\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss:     0.6155  Validation Accuracy: 0.850000\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:     0.4952  Validation Accuracy: 0.825000\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss:     0.6351  Validation Accuracy: 0.850000\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss:     0.5865  Validation Accuracy: 0.850000\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.6291  Validation Accuracy: 0.775000\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:     0.6491  Validation Accuracy: 0.750000\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss:     0.4794  Validation Accuracy: 0.875000\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:     0.5770  Validation Accuracy: 0.875000\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:     0.5971  Validation Accuracy: 0.825000\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.6443  Validation Accuracy: 0.750000\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:     0.6045  Validation Accuracy: 0.750000\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:     0.5158  Validation Accuracy: 0.900000\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:     0.6423  Validation Accuracy: 0.850000\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:     0.5933  Validation Accuracy: 0.850000\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.6079  Validation Accuracy: 0.775000\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:     0.5925  Validation Accuracy: 0.800000\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:     0.4824  Validation Accuracy: 0.900000\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:     0.5895  Validation Accuracy: 0.900000\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:     0.6079  Validation Accuracy: 0.900000\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.6726  Validation Accuracy: 0.750000\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:     0.5906  Validation Accuracy: 0.775000\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:     0.4570  Validation Accuracy: 0.875000\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:     0.6100  Validation Accuracy: 0.875000\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:     0.5676  Validation Accuracy: 0.925000\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.6047  Validation Accuracy: 0.775000\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:     0.5753  Validation Accuracy: 0.850000\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:     0.4491  Validation Accuracy: 0.900000\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:     0.6271  Validation Accuracy: 0.800000\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:     0.5822  Validation Accuracy: 0.875000\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.6079  Validation Accuracy: 0.800000\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:     0.5623  Validation Accuracy: 0.850000\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:     0.4921  Validation Accuracy: 0.925000\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:     0.5785  Validation Accuracy: 0.825000\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:     0.5703  Validation Accuracy: 0.900000\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.6215  Validation Accuracy: 0.800000\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:     0.5688  Validation Accuracy: 0.800000\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:     0.4761  Validation Accuracy: 0.850000\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:     0.5742  Validation Accuracy: 0.850000\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:     0.5629  Validation Accuracy: 0.925000\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.6307  Validation Accuracy: 0.800000\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:     0.6299  Validation Accuracy: 0.800000\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:     0.4750  Validation Accuracy: 0.900000\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:     0.5587  Validation Accuracy: 0.900000\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:     0.5412  Validation Accuracy: 0.900000\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.6355  Validation Accuracy: 0.800000\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:     0.5958  Validation Accuracy: 0.875000\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:     0.4707  Validation Accuracy: 0.900000\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:     0.5873  Validation Accuracy: 0.875000\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:     0.5546  Validation Accuracy: 0.950000\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.5798  Validation Accuracy: 0.800000\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:     0.6283  Validation Accuracy: 0.750000\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:     0.4705  Validation Accuracy: 0.875000\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:     0.5630  Validation Accuracy: 0.875000\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:     0.5635  Validation Accuracy: 0.900000\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.5778  Validation Accuracy: 0.850000\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:     0.6297  Validation Accuracy: 0.800000\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:     0.4385  Validation Accuracy: 0.875000\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:     0.5651  Validation Accuracy: 0.875000\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:     0.5836  Validation Accuracy: 0.875000\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.5614  Validation Accuracy: 0.850000\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:     0.5903  Validation Accuracy: 0.825000\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:     0.5010  Validation Accuracy: 0.875000\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:     0.5671  Validation Accuracy: 0.900000\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:     0.5679  Validation Accuracy: 0.850000\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.6281  Validation Accuracy: 0.800000\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:     0.5829  Validation Accuracy: 0.825000\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:     0.4866  Validation Accuracy: 0.825000\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:     0.5600  Validation Accuracy: 0.850000\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:     0.5452  Validation Accuracy: 0.900000\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.5631  Validation Accuracy: 0.800000\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:     0.5372  Validation Accuracy: 0.875000\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:     0.4848  Validation Accuracy: 0.825000\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:     0.5578  Validation Accuracy: 0.825000\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:     0.5550  Validation Accuracy: 0.875000\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.5473  Validation Accuracy: 0.800000\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:     0.5850  Validation Accuracy: 0.825000\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss:     0.4544  Validation Accuracy: 0.850000\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:     0.5793  Validation Accuracy: 0.875000\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:     0.5468  Validation Accuracy: 0.950000\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.5922  Validation Accuracy: 0.825000\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss:     0.6064  Validation Accuracy: 0.775000\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss:     0.4689  Validation Accuracy: 0.850000\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss:     0.5676  Validation Accuracy: 0.825000\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss:     0.5574  Validation Accuracy: 0.875000\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.5150  Validation Accuracy: 0.850000\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss:     0.5501  Validation Accuracy: 0.825000\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss:     0.4420  Validation Accuracy: 0.875000\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss:     0.5662  Validation Accuracy: 0.900000\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss:     0.5642  Validation Accuracy: 0.875000\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.5799  Validation Accuracy: 0.800000\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss:     0.5631  Validation Accuracy: 0.825000\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss:     0.4346  Validation Accuracy: 0.850000\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss:     0.5564  Validation Accuracy: 0.900000\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss:     0.5467  Validation Accuracy: 0.900000\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.5349  Validation Accuracy: 0.800000\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss:     0.5405  Validation Accuracy: 0.850000\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss:     0.4526  Validation Accuracy: 0.850000\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss:     0.5680  Validation Accuracy: 0.875000\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss:     0.5186  Validation Accuracy: 0.925000\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.5977  Validation Accuracy: 0.825000\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss:     0.5479  Validation Accuracy: 0.800000\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss:     0.4302  Validation Accuracy: 0.900000\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss:     0.5406  Validation Accuracy: 0.875000\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss:     0.5222  Validation Accuracy: 0.875000\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.5210  Validation Accuracy: 0.800000\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss:     0.5463  Validation Accuracy: 0.850000\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss:     0.4496  Validation Accuracy: 0.900000\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss:     0.5736  Validation Accuracy: 0.850000\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss:     0.5544  Validation Accuracy: 0.925000\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.5468  Validation Accuracy: 0.800000\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss:     0.5282  Validation Accuracy: 0.850000\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss:     0.4346  Validation Accuracy: 0.925000\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss:     0.5600  Validation Accuracy: 0.800000\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss:     0.5497  Validation Accuracy: 0.850000\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.5808  Validation Accuracy: 0.800000\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss:     0.5155  Validation Accuracy: 0.850000\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss:     0.4386  Validation Accuracy: 0.900000\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss:     0.6051  Validation Accuracy: 0.825000\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss:     0.5126  Validation Accuracy: 0.900000\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.5108  Validation Accuracy: 0.825000\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss:     0.5357  Validation Accuracy: 0.825000\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss:     0.4629  Validation Accuracy: 0.875000\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss:     0.5979  Validation Accuracy: 0.850000\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss:     0.5247  Validation Accuracy: 0.900000\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.4610  Validation Accuracy: 0.825000\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss:     0.5226  Validation Accuracy: 0.775000\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss:     0.4486  Validation Accuracy: 0.925000\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss:     0.5644  Validation Accuracy: 0.850000\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss:     0.5262  Validation Accuracy: 0.850000\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.5658  Validation Accuracy: 0.800000\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss:     0.6143  Validation Accuracy: 0.825000\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss:     0.4223  Validation Accuracy: 0.900000\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss:     0.5246  Validation Accuracy: 0.875000\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss:     0.4869  Validation Accuracy: 0.900000\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.4960  Validation Accuracy: 0.875000\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss:     0.5138  Validation Accuracy: 0.875000\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss:     0.4291  Validation Accuracy: 0.925000\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss:     0.5085  Validation Accuracy: 0.875000\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss:     0.4969  Validation Accuracy: 0.925000\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.5357  Validation Accuracy: 0.800000\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss:     0.5359  Validation Accuracy: 0.825000\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss:     0.4318  Validation Accuracy: 0.900000\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss:     0.5073  Validation Accuracy: 0.900000\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss:     0.4980  Validation Accuracy: 0.950000\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.5005  Validation Accuracy: 0.850000\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss:     0.5451  Validation Accuracy: 0.750000\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss:     0.3822  Validation Accuracy: 0.850000\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss:     0.5243  Validation Accuracy: 0.900000\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss:     0.4990  Validation Accuracy: 0.900000\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.5022  Validation Accuracy: 0.850000\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss:     0.5394  Validation Accuracy: 0.825000\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss:     0.4035  Validation Accuracy: 0.900000\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss:     0.5590  Validation Accuracy: 0.875000\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss:     0.4839  Validation Accuracy: 0.900000\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.4952  Validation Accuracy: 0.850000\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss:     0.5460  Validation Accuracy: 0.850000\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss:     0.4024  Validation Accuracy: 0.975000\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss:     0.5065  Validation Accuracy: 0.925000\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss:     0.4956  Validation Accuracy: 0.950000\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.5137  Validation Accuracy: 0.800000\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss:     0.5387  Validation Accuracy: 0.825000\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss:     0.4214  Validation Accuracy: 0.925000\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss:     0.5188  Validation Accuracy: 0.900000\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss:     0.5034  Validation Accuracy: 0.875000\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.5082  Validation Accuracy: 0.825000\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss:     0.5405  Validation Accuracy: 0.875000\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss:     0.3902  Validation Accuracy: 0.975000\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss:     0.5009  Validation Accuracy: 0.875000\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss:     0.5191  Validation Accuracy: 0.875000\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.5048  Validation Accuracy: 0.825000\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss:     0.5444  Validation Accuracy: 0.850000\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss:     0.4691  Validation Accuracy: 0.875000\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss:     0.4779  Validation Accuracy: 0.925000\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss:     0.4741  Validation Accuracy: 0.900000\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.4943  Validation Accuracy: 0.850000\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss:     0.5310  Validation Accuracy: 0.850000\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss:     0.4191  Validation Accuracy: 0.925000\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss:     0.5258  Validation Accuracy: 0.825000\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss:     0.4615  Validation Accuracy: 0.900000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.6920490506329114\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkUiTYxERERGR6P8DGM9QeD63tNQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f30ea5b4f60>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
